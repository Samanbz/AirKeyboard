{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3971bf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061ca7ae",
   "metadata": {},
   "source": [
    "##### CNN Feature Extractor\n",
    "* Input of size 6x4x10 (cxhxw)\n",
    "* -> zero pad of 1 pixel -> 6x6x12\n",
    "* -> conv1 32x3x2 s 1x1 -> 32x4x10 (remove column 5)\n",
    "* -> bn1\n",
    "* -> relu1\n",
    "* -> horizontal (left-right) zero pad of 1 pixel -> 32x4x12\n",
    "* -> conv2 64x2x2 s 2x1 -> 64x2x10\n",
    "* -> bn2\n",
    "* -> relu2\n",
    "* -> conv3 1x1x1 s 1x1 -> 1x2x10 (bottleneck)\n",
    "* -> bn3\n",
    "* -> relu3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd773906",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, output_dim=128):\n",
    "        super(CNNFeatureExtractor, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=6, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=5, kernel_size=(5,2), stride=(1,2))\n",
    "        self.bn3 = nn.BatchNorm2d(5)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        assert x.shape == (x.size(0), 6, 10, 4), f\"Expected input shape (batch_size, 6, 10, 4), got {x.shape}\"\n",
    "\n",
    "        left = x[:,:,:5,:]\n",
    "        right = x[:,:,5:,:]\n",
    "        # Mirror right hand\n",
    "        right = torch.flip(right, dims=[2])\n",
    "\n",
    "        # Concatenate along the batch dimension\n",
    "        x = torch.cat([left, right], dim=0)  # Shape: (2*batch_size, 6, 5, 4)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "\n",
    "        # Split back into left and right\n",
    "        batch_size = x.size(0) // 2\n",
    "        left, right = x[:batch_size], x[batch_size:]\n",
    "\n",
    "        # Mirror right hand back\n",
    "        right = torch.flip(right, dims=[2])\n",
    "\n",
    "        # Concatenate along the width dimension\n",
    "        x = torch.cat([left, right], dim=2)  # Shape: (batch_size, 5, 2, 2)\n",
    "        x = x.transpose(3,1).contiguous() # Shape: (batch_size, 2, 2, 5)\n",
    "        x = x.view(batch_size, -1)  # Shape: (batch_size, 20)\n",
    "\n",
    "        return x\n",
    "\n",
    "class AirKeyboardModel(nn.Module):\n",
    "    def __init__(self, lstm_hidden_size=256, lstm_layers=2):\n",
    "        super(AirKeyboardModel, self).__init__()\n",
    "        self.cnn = CNNFeatureExtractor()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=20,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "\n",
    "        self.output_matrix_rows = 6\n",
    "        self.output_matrix_cols = 15\n",
    "        output_features = self.output_matrix_rows * self.output_matrix_cols\n",
    "\n",
    "        self.classifier = nn.Linear(lstm_hidden_size, output_features)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Input x shape: (batch_size, seq_len, C, W, H)\n",
    "        batch_size, seq_len, c, w, h = x.shape\n",
    "\n",
    "        # Pass through CNN\n",
    "        cnn_in = x.view(batch_size * seq_len, c, w, h)\n",
    "        cnn_out = self.cnn(cnn_in)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        lstm_in = cnn_out.view(batch_size, seq_len, -1)\n",
    "        lstm_out, _ = self.lstm(lstm_in) # Shape: (batch_size, seq_length, 256)\n",
    "        raw_output = self.classifier(lstm_out) # Shape: (batch_size, seq_size, 90)\n",
    "        normalized_output = self.log_softmax(raw_output)\n",
    "        output = normalized_output.view(batch_size, seq_len, self.output_matrix_rows, self.output_matrix_cols)\n",
    "\n",
    "\n",
    "        return output\n",
    "\n",
    "class JointSequenceDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, valid_indices: list, seq_length=30):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        self.valid_indices = valid_indices # The pre-computed list of starting indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Use idx to look up the real starting index in the DataFrame\n",
    "        # This line will now be safe because idx will never be larger than len(self.valid_indices) - 1\n",
    "        start_idx = self.valid_indices[idx]\n",
    "        \n",
    "        seq_data = self.data.iloc[start_idx : start_idx + self.seq_length]\n",
    "        \n",
    "        # Ensure your data columns are named 'X' and 'y1' as per your original code\n",
    "        X_seq_np = np.array(seq_data['X'].tolist())\n",
    "        y_seq_np = np.array(seq_data['y1'].tolist())\n",
    "\n",
    "        X_seq = torch.as_tensor(X_seq_np, dtype=torch.float32)\n",
    "        y_seq = torch.as_tensor(y_seq_np, dtype=torch.float32)\n",
    "\n",
    "        return X_seq, y_seq\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02006ad3",
   "metadata": {},
   "source": [
    "### The loss function: Manhattan distance over keybaord layout\n",
    "Layout\n",
    "```plaintext\n",
    "ESC    F1     F2     F3     F4     F5     F6     F7     F8     F9     F10    F11    F12    INS    DEL\n",
    "`      1      2      3      4      5      6      7      8      9      0      -      =      BKSP   BKSP\n",
    "TAB    Q      W      E      R      T      Y      U      I      O      P      [      ]      ENTR   ENTR\n",
    "CPSL   A      S      D      F      G      H      J      K      L      ;      '      \\      ENTR   ENTR\n",
    "LSHFT  \\      Z      X      C      V      B      N      M      ,      .      /      RSHFT  RSHFT  RSHFT\n",
    "LCTRL  FN     META   LALT   SPC    SPC    SPC    SPC    SPC    RALT   RCTRL  LEFT   UP     DOWN   RIGHT\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8dd471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorizing training sequences...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(114132, 91186, 22708, 0, 91186)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# He (Kaiming) Weight Initialization function\n",
    "def weights_init(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif \"weight_hh\" in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "                \n",
    "# HYPERPARAMETERS\n",
    "SEQ_LENGTH = 120\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "df = pd.read_hdf(\"processed_data_17551910634500783_17578807586533651.h5\")\n",
    "df[\"key_present\"] = df[\"pressed_label\"].apply(lambda x: 0 if x == \"NO_KEY\" else 1)\n",
    "\n",
    "split_point = int(0.8 * len(df))\n",
    "train_df = df.iloc[:split_point]\n",
    "val_df = df.iloc[split_point:]\n",
    "\n",
    "# --- Apply downsampling logic ONLY to train_df ---\n",
    "train_start_indices = range(len(train_df) - SEQ_LENGTH + 1)\n",
    "minority_train_indices = []\n",
    "majority_train_indices = []\n",
    "\n",
    "logging.info(\"Categorizing training sequences...\")\n",
    "for i in train_start_indices:\n",
    "    if train_df['key_present'].iloc[i : i + SEQ_LENGTH].to_numpy().sum() > 0:\n",
    "        minority_train_indices.append(i)\n",
    "    else:\n",
    "        majority_train_indices.append(i)\n",
    "\n",
    "\n",
    "# Downsample the majority indices from the training set\n",
    "num_majority_to_keep = len(minority_train_indices) * 8\n",
    "if len(majority_train_indices) < num_majority_to_keep:\n",
    "    num_majority_to_keep = len(majority_train_indices)\n",
    "majority_indices_downsampled = np.random.choice(\n",
    "    majority_train_indices, \n",
    "    size=num_majority_to_keep, \n",
    "    replace=False\n",
    ").tolist()\n",
    "\n",
    "# The final list of indices for training is balanced\n",
    "final_train_indices = minority_train_indices + majority_indices_downsampled\n",
    "np.random.shuffle(final_train_indices)\n",
    "\n",
    "train_dataset = JointSequenceDataset(df, final_train_indices, seq_length=SEQ_LENGTH)\n",
    "\n",
    "# The validation indices are just all possible sequences in the imbalanced val_df\n",
    "all_val_indices = list(range(len(val_df) - SEQ_LENGTH + 1))\n",
    "\n",
    "# Create the two separate datasets\n",
    "train_dataset = JointSequenceDataset(train_df, final_train_indices, seq_length=SEQ_LENGTH)\n",
    "val_dataset = JointSequenceDataset(val_df, all_val_indices, seq_length=SEQ_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "len(df), len(train_dataset), len(val_dataset), len(majority_train_indices), len(minority_train_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80bc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msamanbzg\u001b[0m (\u001b[33msamanbzg-tu-darmstadt\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\saman\\Dev\\AirKeyboard\\wandb\\run-20250917_110805-eq8san4l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/samanbzg-tu-darmstadt/air-keyboard-project/runs/eq8san4l' target=\"_blank\">scarlet-terrain-1</a></strong> to <a href='https://wandb.ai/samanbzg-tu-darmstadt/air-keyboard-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/samanbzg-tu-darmstadt/air-keyboard-project' target=\"_blank\">https://wandb.ai/samanbzg-tu-darmstadt/air-keyboard-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/samanbzg-tu-darmstadt/air-keyboard-project/runs/eq8san4l' target=\"_blank\">https://wandb.ai/samanbzg-tu-darmstadt/air-keyboard-project/runs/eq8san4l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] | Train Loss: 3.1037 | Val Loss: 3.1199 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100] | Train Loss: 2.9006 | Val Loss: 3.1103 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100] | Train Loss: 2.8984 | Val Loss: 3.1132 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100] | Train Loss: 2.8917 | Val Loss: 3.0884 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100] | Train Loss: 2.5538 | Val Loss: 2.8824 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100] | Train Loss: 2.1307 | Val Loss: 2.9307 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100] | Train Loss: 1.9615 | Val Loss: 3.0146 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100] | Train Loss: 1.8951 | Val Loss: 2.8997 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100] | Train Loss: 1.8290 | Val Loss: 2.9053 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100] | Train Loss: 1.7998 | Val Loss: 2.8505 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100] | Train Loss: 1.7900 | Val Loss: 2.8891 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100] | Train Loss: 1.7356 | Val Loss: 2.9305 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100] | Train Loss: 1.7456 | Val Loss: 2.9178 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100] | Train Loss: 1.7052 | Val Loss: 2.9220 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100] | Train Loss: 1.7149 | Val Loss: 2.9641 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100] | Train Loss: 1.6664 | Val Loss: 2.8623 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100] | Train Loss: 1.6506 | Val Loss: 2.8807 | LR: 0.001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "# INITIALIZE MODEL, LOSS, AND OPTIMIZER\n",
    "model = AirKeyboardModel(lstm_hidden_size=256, lstm_layers=2)\n",
    "model.to(device)\n",
    "model.apply(weights_init)\n",
    "\n",
    "loss_function = nn.L1Loss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=1)\n",
    "\n",
    "wandb.init(\n",
    "    project=\"air-keyboard-project\",\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"epochs\": NUM_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"loss_function\": \"L1Loss\",\n",
    "        \"scheduler\": \"ExponentialLR\"\n",
    "    }\n",
    ")\n",
    "wandb.watch(model, log='all', log_freq=100) # Log every 100 batches\n",
    "\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    \n",
    "    for sequences, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [T]\", leave=False):\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    scheduler.step()\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in val_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            outputs = model(sequences)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    \n",
    "    # Log everything to W&B in one step\n",
    "    # The gradient/weight norms will be logged automatically by wandb.watch\n",
    "    log_dict = {\n",
    "        \"Loss/Train\": avg_train_loss,\n",
    "        \"Loss/Validation\": avg_val_loss,\n",
    "        \"Learning Rate\": lr,\n",
    "        \"epoch\": epoch # Custom x-axis\n",
    "    }\n",
    "    \n",
    "    # average gradient/weight ratio for the whole model\n",
    "    total_grad_norm = 0.0\n",
    "    total_weight_norm = 0.0\n",
    "    for param in model.parameters():\n",
    "        if param.grad is not None:\n",
    "            total_grad_norm += param.grad.data.norm(2).item() ** 2\n",
    "            total_weight_norm += param.data.norm(2).item() ** 2\n",
    "    \n",
    "    total_grad_norm = total_grad_norm ** 0.5\n",
    "    total_weight_norm = total_weight_norm ** 0.5\n",
    "    \n",
    "    # weight gradient to weight ratio\n",
    "    log_dict[\"Gradient-Weight Ratio\"] = total_grad_norm / (total_weight_norm + 1e-8)\n",
    "    \n",
    "    wandb.log(log_dict)\n",
    "\n",
    "    logging.info(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | LR: {lr:.6f}\")\n",
    "\n",
    "logging.info(\"\\nTraining finished.\")\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airkeyboard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
