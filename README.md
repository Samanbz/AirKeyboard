## AirKeyboard
Real-time vision-based keystroke prediction for better typing UX

#### Problem
A significant disparity exists in the user experience of typing on a full-size physical keyboard compared to the input methods available in virtual reality (VR), mobile, and smart TV interfaces.

#### Goal
Predict keystrokes in real time given a video stream of the hands.

#### Model Overview
<img width="1527" height="1031" alt="model_overview" src="https://github.com/user-attachments/assets/7937ae53-011a-426b-87ec-04329ffd0c82" />
<img width="2810" height="841" alt="cnn" src="https://github.com/user-attachments/assets/4264cda2-29dd-4f3c-8ff1-98e6af086346" />
<img width="1598" height="1029" alt="lstm" src="https://github.com/user-attachments/assets/98a069b0-fe0d-43e8-92a7-79c619d76815" />

### Input
<img width="1349" height="1070" alt="input" src="https://github.com/user-attachments/assets/b97e7551-a528-44f0-acae-cb43d0447cb3" />

### Output
<img width="1460" height="740" alt="output1" src="https://github.com/user-attachments/assets/10c2942e-c6d6-4e04-ad65-a626ba66689f" />
<img width="379" height="611" alt="output2" src="https://github.com/user-attachments/assets/4e623c02-1b20-4098-892c-306c263de615" />
